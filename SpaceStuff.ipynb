{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spacy and Textacy stuff\n",
    "spaCy is a text processing package in Python. It slurps up the text and does lots of things to it, one thing being lemmatizations. textacy is a wrapper for spaCy that makes it simple to access lots of the data that Spacy makes. As you can see above, several things can be accomplished by a single line. Working on the whole novel The Waves, it takes about 15 seconds and some windup on the hard drive to do anything, so it's a bit slow. On the other hand, it gets the stuff out in pretty good form. I installed spacy using Anaconda, then had to do textacy from the prompt using conda. The respective program documentations describes how to do that. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import textacy\n",
    "import re\n",
    "\n",
    "\n",
    "\n",
    "#nlp = spacy.load(\"en\")           #ok this works to get the ords and such. But there might be a better method.\n",
    "#spaceDoc = nlp(document)\n",
    "#hitlist = []                #make a list of hits\n",
    "#for t in spaceDoc:                   # t.is_alpha = not punctuation, t.sent = word's sentence\n",
    "#    if t.text == targetWord:\n",
    "#        hitlist.append(t.i)\n",
    "#print(hitlist)\n",
    "#for hit in hitlist:\n",
    "#    phrase = []                  #keep a list of tokens in the phrase\n",
    "#    for j in range(hit-targetWindow, hit+targetWindow):\n",
    "#        if spaceDoc[j].is_alpha and spaceDoc[j].lemma_ != \"-PRON-\":\n",
    "#            #print(spaceDoc[j].lemma_)\n",
    "#            phrase.append(spaceDoc[j].lemma_)\n",
    "#    \n",
    "#    print(\" \".join(word for word in phrase))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quotes = textacy.extract.direct_quotations(doc)  #gets a rough list of quotations. Good first pass.\n",
    "for quote in quotes:\n",
    "    print(quote)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sent in doc.sents:    #prints sentences that have the target word in them\n",
    "    for word in sent:\n",
    "        if word.text == targetWord:\n",
    "            print(sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This last command below creates a set of subj verb obj triples for each sentence. The catch is that it works only on transitive sentences. It's pretty good though. Ideally one would have it run on intransitives and transitives, then also on embedded clauses and possible even nonfinite clauses for a complete view of agency. But again its a good easy start. I should add that these textacy functions all create what Python calls a \"generator\". This is essentially an iterator without the iteration statement. The generator \"yields\" one item at a time, so it can be processed before the next item is presented. For example, the trips variable gives up one trip per go. The for-loop gets each trip in turn, does something to it, then gets the next one. In this case I am just printing, but we can do other things like make lists of all the transitive verbs or ones that take particular subjects, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trips = textacy.extract.subject_verb_object_triples(doc) # makes a generator; transitives only\n",
    "for item in trips:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the catch with getting lemmas is that to get the lemmas spacy needs the parse data, and to get that it needs the sentences, which of course need punctuation to detect. So we have to leave the punctuation in, run it into spacy, then find the target words, get their indexes, count forward and backward _excluding_ punctuation, get the lemmas for those indexes, then do the math."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
